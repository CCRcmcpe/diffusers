import argparse

parser = argparse.ArgumentParser(
    description="(Not so) simple example of a training script."
)
parser.add_argument(
    "--pretrained_model_name_or_path",
    type=str,
    required=True,
    help="Path to pretrained model or model identifier from huggingface.co/models."
)
parser.add_argument(
    "--pretrained_vae_name_or_path",
    type=str,
    default=None,
    help="Path to pretrained vae or vae identifier from huggingface.co/models."
)
parser.add_argument(
    "--tokenizer_name",
    type=str,
    default=None,
    help="Pretrained tokenizer name or path if not the same as model_name"
)
parser.add_argument(
    "--instance_data_dir",
    type=str,
    default=None,
    help="A folder containing the training data of instance images."
)
parser.add_argument(
    "--class_data_dir",
    type=str,
    default=None,
    help="A folder containing the training data of class images."
)
parser.add_argument(
    "--instance_prompt",
    type=str,
    default="",
    help="The prompt with identifier specifying the instance"
)
parser.add_argument(
    "--class_prompt",
    type=str,
    default="",
    help="The prompt to specify images in the same class as provided instance images."
)
parser.add_argument(
    "--class_negative_prompt",
    type=str,
    default=None,
    help="The negative prompt to specify images in the same class as provided instance images."
)

parser.add_argument(
    "--save_sample_prompt",
    type=str,
    default=None,
    help="The prompt used to generate sample outputs to save."
)
parser.add_argument(
    "--save_sample_negative_prompt",
    type=str,
    default=None,
    help="The negative prompt used to generate sample outputs."
)
parser.add_argument(
    "--n_save_sample",
    type=int,
    default=4,
    help="The number of samples to save."
)

parser.add_argument(
    "--guidance_scale",
    type=float,
    default=11,
    help="CFG for save sample and class images generation."
)
parser.add_argument(
    "--infer_steps",
    type=int,
    default=28,
    help="The number of inference steps for save sample and class images generation."
)
parser.add_argument(
    "--infer_batch_size",
    type=int,
    default=4,
    help="Batch size (per device) for save sample and class images generation."
)

parser.add_argument(
    "--pad_tokens",
    default=True,
    action="store_true",
    help="Flag to pad tokens to length 77."
)
parser.add_argument(
    "--with_prior_preservation",
    default=False,
    action="store_true",
    help="Flag to add prior preservation loss."
)
parser.add_argument(
    "--prior_loss_weight",
    type=float,
    default=1.0,
    help="The weight of prior preservation loss."
)
parser.add_argument(
    "--num_class_images",
    type=int,
    default=100,
    help=("Minimal class images for prior preservation loss. If not have enough images, additional images will be"
          " sampled with class_prompt.")
)
parser.add_argument(
    "--output_dir",
    type=str,
    default="output",
    help="The output directory."
)
parser.add_argument(
    "--run_id",
    type=str,
    default=None,
    help="Id of this run, random generated by default. Checkpoint will be saved at <output dir>/<run id>/<step>"
)
parser.add_argument(
    "--seed",
    type=int,
    default=None,
    help="A seed for (not so) reproducible training."
)
parser.add_argument(
    "--resolution",
    type=int,
    default=512,
    help=("The resolution for input images, all the images in the train/validation dataset will be resized to this"
          " resolution")
)
parser.add_argument(
    "--center_crop",
    action="store_true",
    help="Whether to center crop images before resizing to resolution"
)
parser.add_argument(
    "--train_text_encoder",
    action="store_true",
    help="Whether to train the text encoder"
)
parser.add_argument(
    "--train_batch_size",
    type=int,
    default=4,
    help="Batch size (per device) for the training dataloader."
)
parser.add_argument(
    "--max_train_steps",
    type=int,
    default=None,
    help="Number of training steps to perform in this run. If provided, overrides num_train_epochs."
)
parser.add_argument(
    "--num_train_epochs",
    type=int,
    default=30,
    help="Run to N epochs, includes previously trained (if any)."
)
parser.add_argument(
    "--gradient_accumulation_steps",
    type=int,
    default=1,
    help="Number of updates steps to accumulate before performing a backward/update pass."
)
parser.add_argument(
    "--gradient_checkpointing",
    action="store_true",
    help="Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass."
)

parser.add_argument(
    "--optimizer",
    type=str,
    default="adamw",
    choices=["adamw", "adamw_8bit", "adamw_ds", "sgdm", "sgdm_8bit"],
    help="The optimizer to use. _8bit optimizers require bitsandbytes, _ds optimizers require deepspeed."
)
parser.add_argument(
    "--adam_beta1",
    type=float,
    default=0.9,
    help="The beta1 parameter for the Adam optimizer."
)
parser.add_argument(
    "--adam_beta2",
    type=float,
    default=0.999,
    help="The beta2 parameter for the Adam optimizer."
)
parser.add_argument(
    "--adam_epsilon",
    type=float,
    default=1e-08,
    help="Epsilon value for the Adam optimizer"
)
parser.add_argument(
    "--sgd_momentum",
    type=float,
    default=0.9,
    help="Momentum value for the SGDM optimizer"
)
parser.add_argument(
    "--sgd_dampening",
    type=float,
    default=0,
    help="Dampening value for the SGDM optimizer"
)
parser.add_argument(
    "--weight_decay",
    type=float,
    default=1e-2,
    help="Weight decay to use."
)

parser.add_argument(
    "--learning_rate",
    type=float,
    default=5e-6,
    help="Initial learning rate (after the potential warmup period) to use."
)
parser.add_argument(
    "--scale_lr",
    action="store_true",
    help="Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size."
)
parser.add_argument(
    "--scale_lr_linear",
    action="store_true",
    help="Overrides --scale_lr to not use sqrt scale but directly multiply."
)
parser.add_argument(
    "--lr_scheduler",
    type=str,
    default="constant",
    help=('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
          ' "constant", "constant_with_warmup"]')
)
parser.add_argument(
    "--lr_warmup_steps",
    type=int,
    default=500,
    help="Number of steps for the warmup in the lr scheduler."
)
parser.add_argument(
    "--lr_cycles",
    type=int,
    default=None,
    help='The number of restarts to use. Default is no restarts. Only works with "cosine" and "cosine_with_restarts" lr scheduler.'
)
parser.add_argument(
    "--last_epoch",
    type=int,
    default=-1,
    help='The index of the last epoch for resuming training. Only works with "cosine" and "cosine_with_restarts" lr scheduler.'
)

parser.add_argument(
    "--gradient_clipping",
    action="store_true",
    help="Whether to enable gradient clipping"
)
parser.add_argument(
    "--max_grad_norm",
    default=1.0,
    type=float,
    help="Max gradient norm. Use with --gradient_clipping."
)
parser.add_argument(
    "--save_interval",
    type=int,
    default=None,
    help="Save checkpoint every N totally trained steps. Overrides --save_interval_epochs."
)
parser.add_argument(
    "--save_interval_epochs",
    type=int,
    default=10,
    help="Save checkpoint every N epochs totally trained."
)
parser.add_argument(
    "--save_min_steps",
    type=int,
    default=0,
    help="Start saving checkpoint after N steps in this run."
)
parser.add_argument(
    "--sample_interval",
    type=int,
    default=None,
    help="Save samples every N steps."
)
parser.add_argument(
    "--mixed_precision",
    type=str,
    default="no",
    choices=["no", "fp16", "bf16"],
    help=("Whether to use mixed precision. Choose"
          "between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10."
          "and an Nvidia Ampere GPU.")
)
parser.add_argument(
    "--not_cache_latents",
    action="store_true",
    help="Do not precompute and cache latents from VAE."
)
parser.add_argument(
    "--local_rank",
    type=int,
    default=-1,
    help="For distributed training: local_rank"
)
parser.add_argument(
    "--concepts_list",
    type=str,
    default=None,
    help="Path to json containing multiple concepts, will overwrite parameters like instance_prompt, class_prompt, etc."
)

parser.add_argument(
    "--wandb",
    default=False,
    action="store_true",
    help="Use wandb to watch training process."
)
parser.add_argument(
    "--wandb_project",
    type=str,
    default="SD-Dreambooth-HF",
    help="Project name in your wandb."
)
parser.add_argument(
    "--wandb_sample",
    default=True,
    action="store_true",
    help="Upload samples of saved checkpoints to wandb."
)
parser.add_argument(
    "--wandb_artifact",
    default=False,
    action="store_true",
    help="Upload saved weights to wandb."
)
parser.add_argument(
    "--rm_after_wandb_saved",
    default=False,
    action="store_true",
    help="Remove saved weights from local machine after uploaded to wandb. Useful in colab."
)

parser.add_argument(
    "--save_unet_half",
    default=False,
    action="store_true",
    help="Use half precision to save unet weights, saves storage."
)
parser.add_argument(
    "--clip_skip",
    type=int,
    default=1,
    help="Stop At last [n] layers of CLIP model when training."
)

parser.add_argument(
    "--read_prompt_from_txt",
    type=str,
    default=None,
    choices=["instance", "class", "both"],
    help="Merge with extra prompt from txt."
)
parser.add_argument(
    "--instance_insert_pos_regex",
    type=str,
    default=None,
    help="The regex used to match instance prompt in txt, so instance_prompt will be inserted at the match index"
)
parser.add_argument(
    "--class_insert_pos_regex",
    type=str,
    default=None,
    help="The regex used to match class prompt in txt, so class_prompt will be inserted at the match index"
)

parser.add_argument(
    "--use_aspect_ratio_bucket",
    default=False,
    action="store_true",
    help="Use aspect ratio bucketing as image processing strategy, which may improve the quality of outputs. Use it with --not_cache_latents"
)
parser.add_argument(
    "--debug_arb",
    default=False,
    action="store_true",
    help="Enable debug logging on aspect ratio bucket."
)

parser.add_argument(
    "--resume",
    default=False,
    action="store_true",
    help="Resume training from specified checkpoint (--pretrained_model_name_or_path)."
)
parser.add_argument(
    "--config",
    type=str,
    default=None,
    help="Read args from yaml config file. Command line args have higher priority and will override yaml."
)
